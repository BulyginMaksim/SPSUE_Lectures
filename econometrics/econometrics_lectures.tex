\documentclass[reqno]{article}

\usepackage{fullpage}
\usepackage{eufrak}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage{pscyr}
\usepackage{amsthm, amsmath, amsfonts, amssymb}


%formatting
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm,bindingoffset=0cm]{geometry}
\renewcommand{\baselinestretch}{1.2}
\newcommand{\term}[1]{\textit{#1}}


\setlength{\parindent}{0pt}
\setlength{\parskip}{0.3em}

\usepackage{multicol}
\usepackage{color}

% Languages, fonts and symbols
\usepackage[english,russian]{babel}
\usepackage[T1,T2A]{fontenc}
\usepackage[cp1251]{inputenc}

\usepackage{amsmath}

\usepackage{graphicx}

\usepackage{subcaption}

\usepackage{amssymb}

\usepackage{listings}

\usepackage{float}

\usepackage{dsfont}

\usepackage{ragged2e}

\usepackage{tabularx}




% Теоремы и прочее
\renewcommand\qedsymbol{$\square$}

\theoremstyle{definition}
\newtheorem*{nb}{Замечание}

\theoremstyle{definition}
\newtheorem*{sol}{Решение}

\theoremstyle{definition}
\newtheorem*{exmp}{Пример}

\theoremstyle{definition}
\newtheorem*{exmps}{Примеры}

\theoremstyle{definition}
\newtheorem{exc}{Упражнение}[section]

\theoremstyle{definition}
\newtheorem{thm}{Теорема}[section]

\theoremstyle{definition}
\newtheorem*{defi}{Определение}

\theoremstyle{definition}
\newtheorem{coll}{Следствие}[section]

\theoremstyle{definition}
\newtheorem{state}{Утверждение}[section]


%Content as links
% Для того, чтобы оглавление стало ссылками, убрать комментирование кода ниже.

\usepackage[hidelinks]{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}


% Title
\title{Лекции по эконометрическому моделированию}
\author{Автор конспекта: Булыгин М.Е.\\Лектор: Нерадовская Ю.В.\\СПбГЭУ\\Санкт-Петербург}


% New Symbols
\newcommand*{\divby}{\mathrel{\rotatebox{90}{$\hskip-1pt.{}.{}.$}}}%




\begin{document}
	
	\maketitle
	\tableofcontents
	
	
	%%%%%
	\newpage
	\section{Принципы и методы эконометрического моделирования}
	\subsection{Эконометрическое моделирование как метод моделирования социальноэкономических процессов}
	\textbf{\term{Модель}} (из словаря) -- образец для изготовления чего-нибудь; тип конструкции;
	манекенщик или манекенщица; уменьшенное (или в натуральную величину) воспроизведение или макет
	чего-нибудь; схема какого-нибудь физического объекта или явления.
	
	Требования к модели: <<Модели должны быть настолько простыми, насколько это возможно, но не проще>>
	(приписывают А.Эйнштейну).
	
	Виды моделей:
	\begin{itemize}
		\item Физические;
		\item Аналоговые;
		\item Символические.
	\end{itemize}
	
	\textbf{\term{Математическая модель экономического объекта}} -- это его гомоморфное отображение в виде 
	совокупности уравнений, неравенств, логических отношений, графиков.
	
	\textbf{\term{Экономическая (экономико-математическая) модель}} -- математическая модель, описывающая механизм
	функционирования экономической или социально-экономической системы.
	
	
	Классификация экономических моделей:
	
	\begin{itemize}
		\item Макро- и микроэкономические;
		\item Теоретические и прикладные;
		\item Статические и динамические;
		\item Детерминированные и стохастические.
	\end{itemize}
	
	\textbf{\term{Моделирование}} -- формирование условного образа реальной системы, отражающего 
	лишь некоторые существенные стороны процесса ее функционирования.
	
	Принципы моделирования:
	\begin{itemize}
		\item Информационная достаточность;
		\item Параметризация отдельных подсистем;
		\item Агрегирование;
		\item Осуществимость цели исследования;
		\item Множественность моделей.
	\end{itemize}
	
	Этапы построения экономической модели:
	\begin{enumerate}
		\item Формулировка предмета и цели исследования;
		\item Выделение элементов системы и их характеристик;
		\item Дескриптивное описание связей между элементами или характеристиками;
		\item Формирование математической модели;
		\item Расчеты по модели и анализ решения.
	\end{enumerate}
	
	Характеристики системы:
	\begin{itemize}
		\item Внешняя среда (эндогенная);
		\item Управляющие воздействия (эндогенная);
		\item Внутренние характеристики (эндогенная);
		\item Выходные характеристики (экзогенная).
	\end{itemize}
	
	Экзогенные переменные:
	\begin{itemize}
		\item Управляемые и неуправляемые;
		\item Внешние и внутренние;
		\item Определенные и неопределенные.
	\end{itemize}
	
	\textbf{\term{Неопределенность}} бывает стохастическая и нестохастическая.
	
	Нестохастическая неопределенность:
	\begin{itemize}
		\item Поведенческая;
		\item Гносеологическая;
		\item Метрологическая;
		\item Целевая.
	\end{itemize}
	
	Адекватность модели -- причины неадекватности:
	
	\begin{itemize}
		\item Ошибки в моделировании;
		\item Нестабильность неопределенных факторов;
		\item Недостоверность исходных данных;
		\item Нерепрезентативность выборки.
	\end{itemize}
	
	Оценка адекватности модели:
	
	\begin{itemize}
		\item Ретроспективный анализ;
		\item Логико-математический анализ;
		\item Экспертная оценка.
	\end{itemize}
	
	Оценка точности модели:
	\begin{itemize}
		\item $d_l = \dfrac{\sum\limits_{i=1}^n|y_i - \hat{y}_i|}{n}$
		\item $d_s = \sqrt{\dfrac{\sum\limits_{i=1}^n(y_i - \hat{y}_i)^2}{n}}$
		\item $\overline{A} = \dfrac{1}{n}\sum\limits_{i=1}^n\left|\dfrac{y_i - \hat{y}_i}{y_i}\right|$
		\item $R^2 = \dfrac{\sum\limits_{i=1}^n(\hat{y}_i-\overline{y})^2}{\sum\limits_{i=1}^n(y_i-\overline{y})^2}$
	\end{itemize}
	
	\textbf{\term{Эконометрическая модель}} -- вероятностно-статистическая модель, описывающая механизм функционирования
	экономической или социально-экономической системы.
	
	Паутинообразная модель (в такой форме еще не является эконометрической моделью):
	
	\begin{equation*}
		\begin{cases}
			Q_{\text{предл},t} = f(p_{t-1})\\
			Q_{\text{спрос},t} = g(p_{t})\\
			\lim\limits_{t \rightarrow \infty} f(p_{t-1}) = \lim\limits_{t \rightarrow \infty} g(p_{t-1})
		\end{cases}
	\end{equation*}
	
	
	Паутинообразная модель в форме эконометрической модели:
	
	\begin{equation*}
		\begin{cases}
			Q_{\text{предл},t} = a_1 + b_1 p_{t-1} + e_{1t}\\
			Q_{\text{спрос},t} = a_2 + b_2 p_{t} + e_{2t}\\
			\lim\limits_{t \rightarrow \infty} f(p_{t-1}) = \lim\limits_{t \rightarrow \infty} g(p_{t-1})
		\end{cases}
	\end{equation*}
	
	\textbf{\term{Объект эконометрического моделирования}} -- социально-экономические явления и процессы.
	
	\textbf{\term{Предмет эконометрического моделирования}} -- статистические закономерности, присущие
	исследуемым явлениям и процессам.
	
	\subsection{Этапы эконометрического моделирования}
	
	
	\begin{enumerate}
		\item Постановка задачи:
		\begin{enumerate}[label*=\arabic*.]
			\item Цель исследования:
			\begin{itemize}
				\item Анализ объекта (процесса);
				\item Прогнозирование;
				\item Имитация развития объекта (процесса);
				\item Подготовка управленческих решений;
			\end{itemize}
			\item Показатели и их роль в модели
		\end{enumerate}
		\item Анализ сущности изучаемого объекта;
		\item Сбор и анализ данных;
		\item Построение теоретической модели;
		\item Статистический анализ модели и оценка ее параметров;
		\item Верификация модели.
	\end{enumerate}
	
	\subsection{Сбор и анализ данных}
	
	\begin{itemize}
		\item Сбор данных;
		\item Анализ качества данных;
		\item Отбор показателей в модель.
	\end{itemize}
	
	Сбор данных -- Федеральная служба государственной статистики \textbf{www.gks.ru}.
	
	Подсайты ФСГС:
	\begin{itemize}
		\item Единая межведомственная информационно-статистическая система (ЕМИСС);
		\item Показатели муниципальных образований.
	\end{itemize}
	
	Анализ качества данных:
	\begin{itemize}
		\item <<Аномальные значения>>;
		\item Однородность совокупности;
		\item Периодизация временных рядов;
		\item Полнота данных (цензурированные выборки);
		\item Вид переменных.
	\end{itemize}
	
	Данные с пропусками:
	\begin{itemize}
		\item Восстановление:
		\begin{itemize}
			\item Заменить пропущенные данные на средние по уровню ряда (совокупности);
			\item Построить обратную зависимость независимой переменной ($x$) от зависимой переменной ($y$),
			рассчитывая пропущенные значения;
			\item Построить регрессию независимой переменной ($x$) от других
			экзогенных параметров, рассчитывая пропущенные значения;
		\end{itemize}
		\item Исключение переменной, по которой отсутствует информация;
		\item Особая категория -- <<умышленные пропуски>>.
	\end{itemize}
	
	\subsection{Принципы эконометрического моделирования}
	\textit{\textbf{Принципы}} бывают априорные и апостериорные.
	
	Априорные принципы:
	\begin{itemize}
		\item Теоретическая основа модели;
		\item Достаточное число наблюдений ($n \geqslant 7m$);
		\item Наблюдения являются выборочными данными;
		\item Количественная измеримость;
		\item Стохастические связи;
		\item Однородность связи;
		\item Зависимая переменная (всегда детерминированная часть и стохастическая часть): 
		$y = \hat{y} + \varepsilon$
		\item Регрессионная модель (математическое ожидание СВ-н зависимой переменной лежит на линии регрессии):
		$$M Y_i =f(x_1,x_2,\dots,x_p)=\hat{y}_i$$
		\begin{figure}[h]
			\centering
			\includegraphics[width=0.5\linewidth]{pics/screenshot001}
			\label{fig:screenshot001}
		\end{figure}
		\item Независимые переменные (стохастические, детерминированные).
	\end{itemize}
	Апостериорные принципы:
	\begin{itemize}
		\item Множественность моделей: неуниверсальность конкретной числовой модели;
		\item Оценки параметров как случайные величины;
		\item Критерии выбора модели в условиях теоретической неопределенности;
		\item Постулирование характеристик связи на основе выбранной модели
	\end{itemize}
	
	\subsection{Методы эконометрического моделирования}
	
	Методы эконометрического моделирования:
	\begin{itemize}
		\item Метод наименьших квадратов;
		\item Обобщенный метод наименьших квадратов;
		\item Метод максимального правдоподобия;
		\item Методы построяния моделей временных рядов;
		\item Методы анализа и решения систем эконометрических уравнений.
	\end{itemize}
	
	Другие методы математической и прикладной статистики:
	\begin{itemize}
		\item Статистическая проверка гипотез;
		\item Типологизация и кластеризация социально-экономических объектов: методы дискриминантного анализа,
		кластерного анализа.
	\end{itemize}
	
	\section{Обзор математико-статистического инструментария эконометрического моделирования}
	
	
	\subsection{Методы предварительного анализа данных}
	Методы предварительного анализа данных:
	\begin{itemize}
		\item Графический -- расположение данных на графике;
		\item Логический (исторический) -- предполагает понимание возможных изменений данных связанных с
		изменениями в социально-экономических системах;
		\item Аналитические методы:
		\begin{itemize}
			\item Критерии <<выбросов>> (аномальности);
			\item Кластеризация в случае пространственных данных, периодизация в случае временных рядов;
			\item Объединение данных -- смыкание рядов;
			\item Устранение несопоставимости отдельных уровней временного ряда (сначала нужно понять, что
			какие то уровни временного ряда несопоставимы);
			\item Выявление тенденции, периодических колебаний.
		\end{itemize}
	\end{itemize}
	
	Проблемы выявления <<аномальных>> точек (аномальность понимается по разному в двух случаях):
	\begin{itemize}
		\item $M Y_i = const$ -- нет регрессии, такое может встречаться только во временных рядах 
		(постоянство математического ожидания $\Rightarrow$ стационарность ряда);
		\item $M Y_i = f(x_1,x_2,\dots,x_p)$ -- есть регрессия.
	\end{itemize}
	
	Первый случай -- постоянство математического ожидания:  
	
	$$M Y_i = const$$
	$$D Y_i = const$$
	$$v = \dfrac{\sigma_y}{\overline{y}}\cdot 100\%$$
	
	Где $v$ -- коэффициент вариации (должен быть небольшим). Не совсем удачный коэффициент: часто может быть высоким при однородности совокупности, зависит от единиц измерения.
	
	Вместо коэффициента вариации предлагается проверить нормальность распределения признака $y$ по правилу трех сигм -- если СВ признака $y$
	распределена нормально, то точка, являющаяся аномальной будет выходить за границы следующего доверительного интервала (так как у нормальной случайной величины
	в этом доверительном интервале лежит 99.72\% значений случайной величины):
	$$(\overline{y} - 3\sigma_y;\overline{y} + 3\sigma_y)$$
	
	Второй случай -- непостоянство математического ожидания:
	$$ M Y_i = f(x_1,x_2,\dots,x_p)$$
	$$ D Y_i = const$$
	
	В таком случае возможны две ситуации: или точка является <<влиятельной>>, или точка является <<выбросом>>.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot004}
		\label{fig:screenshot004}
	\end{figure}
	
	Синяя точка существенно отклонилась относительно среднего значения, в данном случае эта точка -- \term{выброс} (далеко от математического ожидания).
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot006}
		\label{fig:screenshot006}
	\end{figure}
	
	МНК посчитает, что тут три <<облака>> точек. Точки, являющиеся вторым и третьим облаком, называют \term{влиятельными}.
	
	\textit{\textbf{Влиятельность}}:
	
	$$\mathit{se(y_{\text{пр}})} = \sqrt{\dfrac{SS_{\text{ост}}}{n-2}\left(1+\dfrac{1}{n} + \dfrac{(x_{\text{пр}} - \overline{x})^2}{\sum_{i=1}^{n}(x_i - \overline{x})}\right)}$$
	
	$$ se(y_{\text{пр}}) = \sqrt{\dfrac{SS_{\text{ост}}}{n-m-1} 1 + X^{T}_{\text{пр}}\cdot(X^T \cdot X)^{-1}\cdot X_{\text{пр}}}$$	
	
	
	$$ h_i = X_i^T \cdot(X^T_{(i)}\cdot X_{(i)})^{-1} \cdot X_i$$
	
	Где $h_i$ -- характеристика влиятельности, а $X_{(i)}$ -- матрица без исследуемого элемента $i$, $X_{i}$ -- вектор, который содержит компоненты точки $i$, которую 
	проверяем на влиятельность: считаем точку влиятельной, если $h_i > \dfrac{2m}{n}$
	
	Для нашего примера:
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\linewidth]{pics/screenshot007}
		\label{fig:screenshot007}
	\end{figure}
	
	$$h_{18} = 1.87, \; h_{\text{кр}} = 0.1$$
	$$h_{19} = 0.056, \; h_{\text{кр}} = 0.1$$
	
	Таким образом, точка 18 является влиятельной, а 19, которая больше походила на выброс, не является влиятельной -- просто выброс.
	
	\textit{\textbf{Выбросы}}:
	$$B = (X^T \cdot X)^{-1} \cdot X^T \cdot Y$$
	$$\widehat{Y} = X \cdot B$$
	$$\widehat{Y} = X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot Y$$
	
	$$ H = X \cdot (X^T \cdot X)^{-1} \cdot X^T$$
	
	Матрица $H$ (размера $n \times n$) является основой для определения выбросов.
	
	$$\widehat{Y} = H \cdot Y$$
	
	Можно показать, что элементы матрицы $H$ являются частью формулы ковариации случайных остатков: если остатки не свзяаны $cov(\varepsilon_i,\varepsilon_j) = 0$, то $h_{ij} = 0$
	(либо близок к нулю), а если  $cov(\varepsilon_i,\varepsilon_i) \not = 0$, то $h_{ii} \not = 0$.
	
	$$\hat{y}_i = y_i \cdot h_{ii} + \sum_{j \not = i}^{} y_j h_{ij}$$
	
	При нулевой ковариации случайных остатков (в случае независимости случайных остатков по следствию Гаусса-Маркова) $cov(\varepsilon_i,\varepsilon_j) = 0$ 
	элементы $h_{ij} = 0$ (либо близки к нулю), поэтому основная 
	<<нагрузка>> на отклонение $\hat{y}_i$ от $y_i $ лежит на элементе $h_{ii}$, который называют \term{элементом влияния}. На его основе рассчитываются меры выбросов. 
	
	Стандартизованные (стьюдентизованные) остатки:
	
	$$ \hat{\varepsilon}(i) = \frac{\hat{\varepsilon}_i}{\sqrt{1-h_{ii}}} \Big/
	\sqrt{
		\frac{SS_\varepsilon - \frac{\hat{\varepsilon}^2_i}{1-h_{ii}}}{n-m-2}}$$
	
	$\hat{\varepsilon}_i$ -- остаток, рассчитанный по уравнению регрессии, параметры которого рассчитаны по всем наблюдениям (получается, выброс сюда входит), $h_{ii}$ -- элемент диагональный
	из матрицы $H$, которая тоже строится по всем наблюдениям, включая предполагаемый выброс, $SS_\varepsilon$ по той же регрессии, включая проверяемое наблюдение. 
	
	Элемент считается выбросом, если $|\hat{\varepsilon}(i)| > 2$
	
	Для нашего примера:
	$$\hat{\varepsilon}(19) = 3.42$$
	$$\hat{\varepsilon}(\text{кр}) = 2$$
	$$\hat{\varepsilon}(18) = -0.29$$
	
	Таким образом, предположения подтвердились: 19 точка оказалась выбросом, а 18 не оказалась выбросом (она была признана влиятельной ранее).
	
	\textit{\textbf{Кластеризация (периодизация)}}:
	\begin{itemize}
		\item Модели с фиктивными переменными -- ANOVA (модели дисперсионного анализа)
		$$y = a + c \cdot z + \varepsilon$$
		\item Модели ANCOVA (модели ковариационного анализа)
		$$ y = a + b\cdot x +c_{11} \cdot z + c_{12} \cdot z \cdot x + \varepsilon$$
	\end{itemize}
	
	В случае незначимости коэффициентов при фиктивных переменных мы говороим об однородности совокупности (абсолютная однородность в случае незначимости всех коэффициентов перед фиктивных
	переменных).
	
	Тест Чоу -- оценка неоднородности совокупности:
	$$ y = a + b_1 \cdot x_1 + \cdots + b_p \cdot x_p + \varepsilon$$
	\begin{itemize}
		\item Регрессия общая: $n = n_1 + n_2$ наблюдений, $SS_{\text{ост}}^{(0)}$
		\item Регрессия (1): $n_1$ наблюдений, $SS_{\text{ост}}^{(1)}$
		\item Регрессия (2): $n_2$ наблюдений, $SS_{\text{ост}}^{(2)}$
	\end{itemize}
	
	Тестирование с помощью критерия Фишера:
	
	$$ F = \dfrac{SS_{\text{ост}}^{(0)} - (SS_{\text{ост}}^{(1)} + SS_{\text{ост}}^{(2)})}{SS_{\text{ост}}^{(1)} + SS_{\text{ост}}^{(2)}} \cdot \dfrac{n-m_1-m_2-2}{m_1+m_2+1-m} $$
	
	$$ F_{\text{табл}}(\alpha; df_1=m_1+m_2+1-m;df_2 = n -m_1-m_2-2)$$
	
	$F \geqslant F_{\text{табл}}$ означает неоднородность совокупности (то есть описывается разными тенденциями), в противном случае -- наоборот.
	
	\textit{\textbf{Метод параллельной периодизации}}: рассмотрим следующий ряд, о котором мы предполагаем неоднородность процесса -- то есть, что нужна периодизация временного ряда.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\linewidth]{pics/screenshot008}
		\label{fig:screenshot008}
	\end{figure}
	
	Метод параллельной периодизации состоит в том, что мы подбираем вторую переменную ($y$ -- первая), которая, с одной стороны, тесно связана с $y$, а, с другой стороны, имеет
	более четкий период. Например, переменная $x$ для примера:
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\linewidth]{pics/screenshot009}
		\label{fig:screenshot009}
	\end{figure}
	
	
	\textit{\textbf{Объединение данных}} -- проблема смыкания рядов динамики. Предположим, что у нас есть следующий временной ряд, в котором с $t = 1$ по $t=5$ показатель рассчитывался по одной методологии --
	$y_t^1$, а с $t=5$ по $t=9$ рассчитывался по другой методологии -- $y_t^2$:
	\begin{center}
		\begin{tabular}{|>{\centering\arraybackslash}m{1cm}|c|c|c|c|c|c|c|c|c|}
			\hline
			& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
			\hline
			$y_t^1$& 10 & 9 & 11 & 13 & 12 &  &  &  &  \\
			\hline
			$y_t^2$&  &  &  &  & 15 & 22 & 20 & 24 & 25 \\
			\hline
		\end{tabular}
	\end{center}
	
	В момент времени $t=5$ показатель был рассчитан по обеим методологиям -- в таком случае можем использовать коэффициент пересчета $K = \dfrac{y_5^2}{y_5^1}$. 
	В данном случае $K = \dfrac{15}{12}=1.25$, по этому коэффициенту пересчитываются старые уровни ряда $y_t^2 = K \cdot y_t^1$:
	
	\begin{center}
		\begin{tabular}{|>{\centering\arraybackslash}m{1cm}|c|c|c|c|c|c|c|c|c|}
			\hline
			& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
			\hline
			$y_t^1$& 10 & 9 & 11 & 13 & 12 &  &  &  &  \\
			\hline
			$y_t^2$& 12.5 & 11.25 & 13.75 & 16.25 & 15 & 22 & 20 & 24 & 25 \\
			\hline
		\end{tabular}
	\end{center}
	
	\textit{\textbf{Устранение несопоставимости отдельных уровней временного ряда}}:
	\begin{itemize}
		\item Пересчет уровней ряда по единой методике;
		\item В единых территориальных и организационных рамках;
		\item В единых ценах (дефлянтирование).
	\end{itemize}
	
	\term{Индексы} -- отношение значений показателя за два периода времени (отчетный уровень к базисному).
	
	Индивидуальные индексы:
	$$i = \dfrac{p_1}{p_0}$$
	
	Сводные индексы:
	$$I = \dfrac{\sum T_1}{\sum T_0}$$
	
	Агрегатные индексы:
	$$I_p= \dfrac{\sum p_1 q_1}{\sum p_0 q_0}$$
	
	Индексы цен (так как нас интересуют дефлянтированные) бывают:
	\begin{itemize}
		\item Ласпейреса:
		$$ I_p= \dfrac{\sum p_1 q_0}{\sum p_0 q_0} $$
		\item Пааше:
		$$I_p= \dfrac{\sum p_1 q_1}{\sum p_0 q_1}$$
		\item Фишера:
		$$ I_p = \sqrt{\dfrac{\sum p_1 q_1}{\sum p_0 q_1} \cdot \dfrac{\sum p_1 q_0}{\sum p_0 q_0}}$$
	\end{itemize}
	
	Цепные индексы:
	$$i_{1/0} = \dfrac{p_1}{p_0}, \; i_{2/1} = \dfrac{p_2}{p_1}, \; i_{3/2} = \dfrac{p_3}{p_2}, \; i_{4/3} = \dfrac{p_4}{p_3}$$
	
	Базисный индекс:
	$$i_{4/0} = \dfrac{p_1}{p_0} \cdot \dfrac{p_2}{p_1} \cdot \dfrac{p_3}{p_2} \cdot \dfrac{p_4}{p_3}$$
	
	Единицы измерения индексов:
	\begin{itemize}
		\item Доли -- для расчетов;
		\item Проценты -- официальная отчетность и информация.
	\end{itemize}
	
	Индекс потребительских цен (ИПЦ) рассчитывается на основе выборочного наблюдения цен, выборка эта по нескольким признакам:
	\begin{itemize}
		\item По населенным пунктам;
		\item По торговым предприятиям;
		\item По товарам и услугам.
	\end{itemize}
	
	Официальные данные, как правило содержат два вида индексов:
	\begin{itemize}
		\item ИПЦ $i/j$:
		$$ I_{i(t)/(i-1)(t)} \; \; \text{или} \; \; I_{1(t)/12(t-1)}, \; i = 1,\cdots,12$$
		\item ИПЦ декабрь к декабрю предыдущего года:
		$$ I_{12(t)/12(t-1)}$$
	\end{itemize}
	
	Обозначение $i(t)$: $i$ -- номер месяца, $(t)$ -- номер года.
	
	Необходимы годовые ИПЦ и квартальные ИПЦ -- для их нахождения необходимо преобразовывать месячные цепные индексы.
	ИПЦ, исходные данные:
	
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			& 1991 & 1992 & 1993 \\
			\hline
			\multicolumn{4}{|c|}{к концу предыдущего месяца} \\
			\hline
			январь & 1.062 & 3.453 & 1.258 \\
			\hline
			февраль & 1.048 & 1.38 & 1.247 \\
			\hline
			март & 1.063 & 1.299 & 1.201 \\
			\hline
			апрель & 1.635 & 1.217 & 1.187 \\
			\hline
			май & 1.93 & 1.119 & 1.181 \\
			\hline
			июнь & 1.012 & 1.191 & 1.199 \\
			\hline
			июль & 1.006 & 1.106 & 1.2239 \\
			\hline
			август & 1.005 & 1.086 & 1.26 \\
			\hline
			сентябрь & 1.011 & 1.115 & 1.23 \\
			\hline
			октябрь & 1.035 & 1.229 & 1.195 \\
			\hline
			ноябрь & 1.089 & 1.261 & 1.1639 \\
			\hline
			декабрь & 1.121 & 1.252 & 1.125 \\
			\hline
			\multicolumn{4}{|c|}{к декабрю предыдущего года:} \\
			\hline
			декабрь & 2.604 & 26.0884 & 9.399 \\
			\hline
		\end{tabular}
	\end{center}
	
	Методика пересчета цепных месячных ИПЦ в цепные годовые:
	\begin{enumerate}
		\item ИПЦ текущего месяца к декабрю предыдущего года:
		$$ I_{i(t)/12(t-1)} = \left(\prod_{j=2}^{i}I_{j(t)/(j-1)(t)}\right) I_{1(t)/12(t-1)}$$
		Для 1991 года:
		$$ I_{\text{янв 91/дек 90}} = 1.062$$
		$$ I_{\text{фев 91/дек 90}} = 1.062 \cdot 1.048 = 1.112976$$
		$$ I_{\text{март 91/дек 90}} = 1.062  \cdot 1.048 \cdot 1.063 = 1.183093$$
		$$ \cdots$$
		$$ I_{\text{дек 91/дек 90}} = 1.062  \cdot \dots \cdot  1.121 = 2.604016$$
		
		В результате получаем:
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				& 1991 & 1992 & 1993 \\
				\hline
				\multicolumn{4}{|c|}{к декабрю предыдущего года} \\
				\hline
				январь & 1.062 & 3.453 & 1.258 \\
				\hline
				февраль & 1.112976 & 4.76514 & 1.568726 \\
				\hline
				март & 1.183093 & 6.189917 & 1.88404 \\
				\hline
				апрель & 1.934358 & 7.533129 & 2.236355 \\
				\hline
				май & 1.992389 & 8.429571 & 2.641136 \\
				\hline
				июнь & 2.016297 & 10.03962 & 3.166722 \\
				\hline
				июль & 2.028395 & 11.10382 & 3.875751 \\
				\hline
				август & 2.038537 & 12.05876 & 4.883446 \\
				\hline
				сентябрь & 2.060961 & 13.4455 & 6.006638 \\
				\hline
				октябрь & 2.133095 & 16.52452 & 7.177933 \\
				\hline
				ноябрь & 2.32294 & 20.83742 & 8.354396 \\
				\hline
				декабрь & 2.604016 & 26.08845 & 9.398696 \\
				\hline
			\end{tabular}
		\end{center}
		\item ИПЦ текущего месяца к декабрю года, предшествующего предыдущему:
		$$I_{i(t)/12(t-2)} = I_{i(t)/12(t-1)} \cdot I_{12(t-1)/12(t-2)} $$
		Например, для 1991 -- нет данных (декабрь 1989), для 1992 -- декабрь 1990, для 1993 -- декабрь 1991.
		
		1992 к декабрю 1990:
		$$ I_{\text{янв 92/дек 90}} = I_{\text{янв 92/дек 91}} \cdot I_{\text{дек 91/дек 90}} $$
		$$ I_{\text{фев 92/дек 90}} = I_{\text{фев 92/дек 91}} \cdot I_{\text{дек 91/дек 90}} $$
		$$ \cdots$$
		
		Для примера:
		$$\text{январь:  } 3.453 \cdot 2.604016 = 8.99166 $$
		$$\text{февраль:  } 4.76514 \cdot 2.604016 = 12.4085 $$
		$$\text{март:  } 6.189917 \cdot 2.604016 = 16.11864 $$
		$$ \cdots$$
		
		В результате получаем:
		\begin{center}
			\begin{tabular}{|c|c|c|c|}
				\hline
				& 1991 & 1992 & 1993 \\
				\hline
				\multicolumn{4}{|c|}{к декабрю предыдущего года} \\
				\hline
				январь &  & 8.991666 & 32.81928 \\
				\hline
				февраль &  & 12.4085 & 40.92564 \\
				\hline
				март & & 16.11864 & 49.15169 \\
				\hline
				апрель & & 19.61639 & 58.34306 \\
				\hline
				май & & 21.95074 & 68.90315 \\
				\hline
				июнь &  & 26.14333 & 82.61488 \\
				\hline
				июль & & 28.914333& 101.1123 \\
				\hline
				август &  & 31.40117 & 127.4016 \\
				\hline
				сентябрь &  & 35.0123 & 156.7039 \\
				\hline
				октябрь & & 43.03012& 187.2612 \\
				\hline
				ноябрь &  & 54.26098 & 217.9533 \\
				\hline
				декабрь &  & 67.93475 & 245.1974 \\
				\hline
			\end{tabular}
		\end{center}
		\item ИПЦ текущего года к предыдщему:
		$$ I_{t/(t-1)} = \dfrac{\sum\limits_{i=1}^{12}I_{i(t)/12(t-2)}}
		{\sum\limits_{i=1}^{12}I_{i(t-1)/12(t-2)}}$$
		Для нашего примера:
		$$I_{1992/1991} = \dfrac{I_{\text{янв 92/дек 90}} + I_{\text{фев 92/дек 90}} + \cdots + I_{\text{дек 92/дек 90}}}
		{I_{\text{янв 91/дек 90}} + I_{\text{фев 91/дек 90}} + \cdots + I_{\text{дек 91/дек 90}}} =$$
		$$ = \dfrac{8.991666 + 12.4085 + \cdots + 67.93475}{1.062 + 1.112976 + \cdots + 2.604016} \approx 16.26 $$
		\item Базисные ИПЦ
		
		Первый год = <<база>>, $I_{1/1} = 1.00$, а следующие годы получаются следующим образом:
		$$ I_{t/(t-k)} = \prod_{j=t-k+1}^{t}I_{j/(j-1)}$$
		Для нашего примера:
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
				\hline
				& 1991 & 1992 & 1993 & 1994\\
				\hline
				ИПЦ цепн & 1.00  & 16.26494 & 9.741572 & 4.076914 \\
				\hline
				ИПЦ баз & 1.00 & 16.26494 & 158.4461 & 645.9709 \\
				\hline
			\end{tabular}
		\end{center}
	\end{enumerate}
	
	Методика пересчета цепных месячных ИПЦ в цепные квартальные:
	\begin{enumerate}
		\item То же самое;
		\item То же самое;
		\item ИПЦ текущего квартала к предыдущему:
		
		$$ I_{t/(t-1)} = \dfrac{\sum\limits_{i=4}^{6}I_{i(t)/12(t-2)}}
		{\sum\limits_{i=1}^{3}I_{i(t)/12(t-2)}}$$
		
		Исключение составляет квартальный индекс первого квартала по сравнению с четвертым кварталом предыдущего года:
		$$ I_{t/(t-1)} = \dfrac{\sum\limits_{i=1}^{3}I_{i(t)/12(t-2)}}
		{\sum\limits_{i=10}^{12}I_{i(t-1)/12(t-2)}}$$
		
		Квартал к предыдщему кварталу для нашего примера:
		
		$$I_{\text{вт.кв. 1992/перв.кв.1992}} = \dfrac{I_{\text{апр 92/дек 90}} + I_{\text{май 92/дек 90}} + I_{\text{июнь 92/дек 90}}}
		{I_{\text{янв 91/дек 90}} + I_{\text{фев 91/дек 90}} + I_{\text{март 91/дек 90}}} =$$
		$$ = \dfrac{19.61639 + 21.95074 + 26.14333}{8.991666 + 12.4085 + 16.11864} \approx 
		1.805 $$
	\end{enumerate}
	
	\textbf{\textit{Выявление тенденции:}}
	\begin{itemize}
		\item Графический метод;
		\item Тест <<восходящих>> и <<нисходящих>> серий;
		\item Проверка постоянства средних и дисперсий;
		\item Анализ автокорреляционной функции;
		\item Оценка значимости уравнения тренда.
	\end{itemize}
	
	\subsection{Отбор факторов в модель регрессии}
	
	Требования к факторам:
	\begin{itemize}
		\item Влияние независимой $x$ переменной на зависимую переменную $y$:
		$$ |r_{yx_i}| \rightarrow 1$$
		\item Отсутствие интеркорреляции (мультиколлинеарность):
		$$
		\Delta_{r_{xx}} = \begin{vmatrix}
			1 & r_{x_1x_2} & \cdots & r_{x_1x_p}\\
			r_{x_1x_2} &1 & \cdots & r_{x_2x_p}\\
			\vdots  & \vdots  & \vdots  & \vdots \\
			r_{x_1 x_p} & r_{x_2x_p} & \cdots &1\\
		\end{vmatrix} \rightarrow 1$$
		\item Выполнение условия того, что факторы должны быть более тесно 
		связаны с результатом,
		чем друг с другом:
		\begin{equation*}
			\begin{cases}
				|r_{yx_i}| > |r_{x_ix_j}|\\
				|r_{yx_j}| > |r_{x_ix_j}|
			\end{cases}
		\end{equation*}
	\end{itemize}
	
	Выявление особенностей факторов:
	\begin{itemize}
		\item Наличие случайной составляющей;
		\item Зависимость от других показателей;
		\item Ложная корреляция с результатом и факторами.
	\end{itemize}
	
	Отбор факторов в уравнение множественной регресии:
	\begin{itemize}
		\item Метод исключения;
		\item Метод включения;
		\item Шаговый регрессионный анализ.
	\end{itemize}
	
	\textit{\textbf{Метод исключения:}}
	\begin{enumerate}
		\item Строится модель со всеми факторами (бывают огранчения на включение в модель
		всех факторов: например, невыполнение $n > 7m$ -- должен быть достаточный объем
		наблюдений);
		\item Оценка значимости включения переменных с помощью $F_x$ (частного
		$F$--критерия, который показывает значимость включения дополнительной переменной,
		который основывается на значимости сокращения остаточной суммы квадратов);
		\item Исключение переменной $x_s$ при $F_{x_s} < F_{\text{крит}}$ 
		и $F_{x_s} = \min F_x$
		\item Пересчет модели;
		\item Переход к шагу 2.
	\end{enumerate}
	
	\textit{\textbf{Шаговый регрессионный анализ:}}
	\begin{enumerate}
		\item Включение первой переменной при условии, что
		$|R_{yx_1}|=\max\limits_{j}(|R_{yx_j}|)$ и 
		$F_{x_1} > F_{\text{крит}}$;
		\item Включение последующих переменных -- максимальный $F_{x_j(x_1\cdots x_{j-1})} 
		> F_{\text{крит}}$;
		\item Пересчет $F_x$ для новой модели;
		\item Если $F_{x_s} < F_{\text{крит}}$ и $F_{x_s} = \min F_x$, то исключение 
		переменной $x_s$ и переход к пункту 3, иначе переход к пункту 2.
	\end{enumerate}
	
	\subsection{Классическая нормальная линейная модель}
	
	Анализ модели:
	\begin{itemize}
		\item Определить виды переменных, входящих в модель: в простейшей модели все переменные количественные;
		\item Функциональная форма модели;
		\item Гипотезы о характере случайных остатков;
		\item Идентифицируемость системы эконометрических уравнений.
	\end{itemize}
	
	Исходя из анализа модели происходит выбор метода оценки параметров модели.
	
	Предпосылки построения классической нормальной линейной модели (КНЛМ):
	
	\begin{enumerate}
		\item $MY_i = f(x_1,\cdots,x_p) = \hat{y}_i$, где $f(x_1,\cdots, x_p) = b_0 + b_1 \cdot x_1 + \cdots b_p \cdot x_p + \varepsilon$
		\item $\sigma^2_{Y_i} = \sigma^2_{Y_j} = \sigma^2_{Y} = const  \;\; \forall i,j$
		\item $Y_i$ имеет нормальное распределение $N(\hat{y}_i, \sigma^2_{Y_i})  \; \forall i$
		\item $r_{Y_iY_j} = 0 \; \forall i,j$
		\item $x_i$ -- неслучайная $\forall i$
	\end{enumerate}
	
	Требования к случайным остаткам КНЛМ (условия Гаусса-Маркова):
	\begin{enumerate}
		\item $M_{\varepsilon_i} = M(Y_i - MY_i) = 0$
		\item $\sigma^2_{\varepsilon_i} = \sigma^2_{\varepsilon_j} = \sigma^2_{\varepsilon} = const \;\; \forall i,j$
		\item $\varepsilon_i \in N(0, \sigma^2_{\varepsilon})  \; \forall i$
		\item $r_{\varepsilon_i\varepsilon_j} = 0 \; \forall i,j$
		\item $r_{\varepsilon_it} = 0 \; \forall i$
	\end{enumerate}
	
	Если данные требования к случайным остаткам выполняются, то оценки МНК являются эффективными в классе несмещенных оценок.
	
	Свойства МНК-оценок при соблюдении предпосылок КНЛМ:
	\begin{itemize}
		\item Несмещенность:
		$$ M\hat{b}_j = b_j$$
		\item Эффективность:
		$$ M(\hat{b}_j - b_j)^2 = \sigma^2_{b_j} = \min \sigma^2_{b_j}$$
		\item Состоятельность:
		$$\hat{b}_j \xrightarrow{n \rightarrow \infty} b_j$$
	\end{itemize}
	
	Проверка качества модели:
	\begin{itemize}
		\item Проверка случаных остатков на постоянство дисперсии;
		\item Проверка значимости параметров;
		\item Проверка значимости функции.
	\end{itemize}
	
	\subsection{Анализ качества случайных остатков}
	
	$$M_{\varepsilon_i} = M(Y_i - MY_i) = 0 \text{ -- выполняется по МНК}$$
	$$\sigma^2_{\varepsilon_i} = \sigma^2_{\varepsilon_j} = \sigma^2_{\varepsilon} = const \;\; \forall i,j$$
	
	Гомоскедастичность тесты:
	\begin{itemize}
		\item Гольдфельда-Квандта
		\item Уайта: $\varepsilon_i^2 = a + b_{11} \cdot x_1 + b_{12} \cdot x_1^2 + \cdots +  b_{p1} \cdot x_p + b_{p2} \cdot x_p^2 + [c_{12} \cdot x_1 \cdot x_2 + \cdots +
		c_{p-1,p} \cdot x_{p-1} \cdot x_p ] + \delta$
		\item Парка: $\ln \varepsilon^2 = a + b \cdot \ln x_j + \delta$
		\item Глейзера: $|\varepsilon| = a + b\cdot x_j^k + \delta \; k = -2,-1,-0.5,0.5,1,2$
		\item Ранговой корреляции Спирмена: $\rho = 1 - \frac{6\sum_{i=1}^{n}d_i^2}{n(n^2-1)}$
	\end{itemize}
	
	$$r_{\varepsilon_i\varepsilon_j} = 0 \; \forall i,j$$
	Проверка независимости случайных остатков:
	\begin{itemize}
		\item Коэффициент автокорреляции
		\item Критерий Дарбина-Уотсона:
		$$ dw = \dfrac{\sum\limits_{i=2}^{n}(\varepsilon_t - \varepsilon_{t-1})^2}{\sum\limits_{i=1}^{n}\varepsilon_t}$$
	\end{itemize}
	$$\varepsilon_i \in N(0, \sigma^2_{\varepsilon})  \; \forall i$$
	
	Проверка нормальности распределения случайных остатков:
	\begin{itemize}
		\item Коэффициенты асимметрии и эксцесса:
		$$ |As| < 1.5 \sqrt{\dfrac{6(n-2)}{(n+1)(n+3)}}$$
		$$ \left|Ex + \dfrac{6}{n+1}\right| < 1.5 \sqrt{\dfrac{24n(n-2)(n-3)}{(n+1)^2(n+3)(n+5)}}$$
		\item RS-критерйи
		\item критерий Жака-Бера
	\end{itemize}
	
	RS-критерий:
	
	$$R = \varepsilon_{max} - \varepsilon_{min}$$ 
	
	$$S = \sqrt{\dfrac{\sum \varepsilon_t^2}{n-1}}$$
	
	$$RS = \dfrac{R}{S}$$
	
	Табличные значения RS-критерия:
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\linewidth]{pics/screenshot011}
		\label{fig:screenshot011}
	\end{figure}
	
	
	Критерий Жака-Бера:
	$$H_0: As = 0; Ex = 0$$
	$$H_1: As \not = 0; Ex \not = 0$$
	$$JB = \dfrac{n-m}{6} \cdot (As^2 + \dfrac{{Ex}^2}{4})$$
	$$\chi^2 (\alpha; 2)$$
	$$JB < \chi^2$$
	
	Пример:
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot012}
		\label{fig:screenshot012}
	\end{figure}
	
	Регрессия по всей совокупности:
	$$y = 109.9 - 1.13\cdot x + \hat{\varepsilon}$$
	
	Нормальность остатков ($n=20$):
	
	$$As = 0.73; \; Ex = 1.16; \; \left|Ex + \dfrac{6}{n+1}\right| = 1.43$$
	$$se(As) = 0.70; \; se(Ex) = 1.13$$
	$$RS = 4.27$$
	$$RS_{max} = 4.49$$
	$$JB = 2.89$$
	$$\chi^2 (0.05; 2) = 5.99$$
	
	Первый критерий не показал нормальность, а 2 и 3 показали нормальность -- в таком случае, ориентируемся на RS-критерий и критерий Жака-Бера и говорим о нормальности распределения случайных остатков.
	
	$$r_{\varepsilon_i t} = 0 \; \forall i \; \text{ -- гипотеза об отсутствии закономерной составляющей}$$
	
	Делаем тесты на стационарность ряда (если временной ряд).
	
	\subsection{Оценка значимости параметров регрессии (тренда)}
	
	Оценки параметров -- случайные величины:
	$$y = b_0 + b_1\cdot x_1 + \cdots + b_j\cdot x_j + \cdots + b_p\cdot x_p + \varepsilon$$
	
	По выборочным данным оцениваем параметры модели:
	$$\hat{b}_j \rightarrow b_j$$
	
	Так как оценка параметра -- случайная величина, возникает вопрос: какой у нее закон распределения? Ответ: оценка параметра имеет нормальный закон распределения, 
	так как по ЦПТ сумма независимых одинаково распределенных случайных величин имеет распределение, близкое к нормальному.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot013}
		\label{fig:screenshot013}
	\end{figure}
	
	$$ \hat{b}_j \in N(b_j, \sigma^2_{b_j})$$
	$$b_j = \; ?$$
	$$\sigma^2_{b_j} = \; ?$$
	
	Введем $b_j = B_j$ ($B_j$ -- число, которое мы предполагаем истинным значением параметра), а $\sigma_{b_j} \rightarrow se(\hat{b}_j)$.
	
	Тогда:
	$$\dfrac{\hat{b}_j - B_j}{se(\hat{b}_j)} \in t(n-m-1)$$
	
	Распределение Стьюдента (для $df=10$):
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot014}
		\label{fig:screenshot014}
	\end{figure}
	
	Например:
	$$y = 2 + 3 \cdot x + \hat{\varepsilon}, \; n = 12$$
	$$\hat{b}_1 = 3, \; se(\hat{b}_1) = 0.5$$
	
	Гипотеза $H_0$:
	$$b_1 = B_1 = 3.0$$
	$$t_{b_1} = \dfrac{3-3}{0.5} = 0$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot015}
		\label{fig:screenshot015}
	\end{figure}
	
	$$P(\hat{b}_1 = b_1) = ?$$
	$$P(\hat{b}_1 = b_1) =P\left(\dfrac{\hat{b}_1 - B_1}{se(\hat{b}_1)} = \dfrac{b_1 - B_1}{se(\hat{b}_1)}\right)=P\left(t_{b_1} = \dfrac{b_1 - B_1}{se(\hat{b}_1)}\right) = P(t_{b_1} = 0) = 0$$
	
	$$P(\hat{b}_1 < b_1) = P\left(\dfrac{\hat{b}_1 - B_1}{se(\hat{b}_1)} < \dfrac{b_1 - B_1}{se(\hat{b}_1)}\right) = P(t_{b_1} < 0) =  0.5$$
	
	Гипотеза $H_0$:
	$$b_1 = B_1 = 2.5$$
	
	$$t_{b_1} = \dfrac{3-2.5}{0.5}=1$$
	
	$$t_{b_1} = 1, df=10$$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot026}
		\label{fig:screenshot026}
	\end{figure}
	
	Можно определить:
	$$ P(t_{b_1}<1) = F(t_{b_1}) = 0.8296$$
	То есть:
	$$ P(t_{b_1}<1) =P\left(\dfrac{\hat{b}_1-b_1}{se(\hat{b}_1)} < \dfrac{\hat{b}_1 - B_1}{se(\hat{b}_1)}\right) = P(b_1 > B_1) = 0.8296 \Rightarrow$$
	$$\Rightarrow P(b_1 \leqslant B_1) = 1 - 0.8296 = 0.1704$$
	
	Продолжение примера:
	$$y=2 + 2 \cdot x + \hat{\varepsilon}, \; n=12$$
	
	$$\hat{b}_1=2, se(\hat{b}_1) = 0.5$$
	
	Гипотеза $H_0$:
	$$b_1 = B_1 = 2.5$$
	$$t_{b_1} = \dfrac{2 - 2.5}{0.5} = -1$$
	
	Можно определить:
	$$ P(t_{b_1}< -1) = F(t_{b_1}) = 0.1704$$
	
	То есть:
	$$ P(t_{b_1}<-1) =P\left(\dfrac{\hat{b}_1-b_1}{se(\hat{b}_1)} < \dfrac{\hat{b}_1 - B_1}{se(\hat{b}_1)}\right) = P(b_1 > B_1) = 0.1704 \Rightarrow$$
	$$\Rightarrow P(b_1 \leqslant B_1) = 1 - 0.1704 = 0.8296$$
	
	Когда говорим об отклонении рассчетного значения $\hat{b}_j$ от гипотетического $b_j$ нас не волнует, в какую сторону произошло отклонение, поэтому, так как распределение симметрично, а $P(t_{b_j} = 0)$ -- идеальный вариант, 
	то нас интересует модуль отклонения  рассчетного значения $|\hat{b}_j|$ от гипотетического $b_j$.
	
	Обобщим два предыдущих случая:
	
	$$P\left(\dfrac{\hat{b}_j-b_j}{se(\hat{b}_j)} < \left|\dfrac{\hat{b}_j - B_j}{se(\hat{b}_j)}\right|\right) = P\left(\dfrac{\hat{b}_j-b_j}{se(\hat{b}_j)} < |t_{b_j}|\right) =
	P\left(-t_{b_j} < \dfrac{\hat{b}_j - b_j}{se(\hat{b}_j)} < t_{b_j}\right) =$$
	$$  = F(t_{b_j}) - F(-t_{b_j}) = P(\hat{b}_j - t_{b_j} \cdot se(\hat{b}_j) < b_j < \hat{b}_j + t_{b_j} \cdot se(\hat{b}_j) )$$
	
	Для нашего примера -- для $|t_{b_j}| = 1$:
	
	$$P(\hat{b}_j - t_{b_j} \cdot se(\hat{b}_j) < b_j < \hat{b}_j + t_{b_j} \cdot se(\hat{b}_j)) = 0.8296 - 0.1704 = 0.6591$$
	
	$$|t_{b_j}| = 1 \;\; P(-1 < t_{b_j} < 1) = 0.6591$$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot028}
		\label{fig:screenshot028}
	\end{figure}
	
	Пусть задано значение $B_j$, тогда $t_{b_j} = f(\hat{b}_j)$.
	
	Пусть $|t_{b_j}| = 1$:
	$$P(-1 < t_{b_j} < 1) = 0.6591$$
	$$P(t_{b_j} > 1) = \dfrac{1 - 0.6591}{2} = 0.17045$$ 
	$$P(t_{b_j} < 1) = \dfrac{1 - 0.6591}{2} = 0.17045$$ 
	$$ 0.17045 +  0.17045 = 0.3409 \equiv \alpha_{b_j}$$
	
	$$P(|t_{b_j}| > 1) = 0.3409$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot029}
		\label{fig:screenshot029}
	\end{figure}
	
	Идея: если верна нулевая гипотеза и коэффициент, оцениваемый, по нашим данным, варьируется вокруг истинного значения, то оставлять такие большие крылья -- неправильно, для этого можем задать бОльшую вероятность попадания в интервал,
	расширив его. Зададим:
	
	$$P(\hat{b}_j - t_{b_j} \cdot se(\hat{b}_j) < b_j < \hat{b}_j + t_{b_j} \cdot se(\hat{b}_j)) = 0.95$$
	
	Или кратко:
	$$P(\hat{b}_{j,\text{мин}} < b_j < \hat{b}_{j,\text{макс}}) = 0.95$$
	
	Будем считать, что если $B_j \in (\hat{b}_{j,\text{мин}};\hat{b}_{j,\text{макс}})$, то $b_j \approx B_j$, а $ (\hat{b}_{j,\text{мин}};\hat{b}_{j,\text{макс}})$ называется \textbf{\term{областью принятия гипотезы}}.
	
	Для нашего примера:
	$$P(-2.228 < t_{b_j} < 2.228) = 0.95$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot030}
		\label{fig:screenshot030}
	\end{figure}
	
	Вероятности хвостов: $\alpha = 1 -0.95 = 0.025 + 0.025 = 0.05$. Гипотеза $b_j = B_j$ отвергается с вероятностью $\alpha = 0.05$, параметр $\alpha$ называется \textbf{\term{уровнем значимости}}.
	
	Незначимость параметра $b_j$:
	$b_j = 0 \in (\hat{b}_{j,\text{мин}};\hat{b}_{j,\text{макс}})$
	
	Если:
	$$b_j = 0$$
	$$\sigma_{\hat{b}_j} \rightarrow se(\hat{b}_j)$$
	
	Тогда:
	
	$$t_{b_j} = \dfrac{\hat{b}_j - 0}{se(\hat{b}_j)} = \dfrac{\hat{b}_j}{se(\hat{b}_j)} \in t(n-m-1)$$
	
	Гипотеза о статистической незначимости параметра $b_j$:
	$$H_0: b_j = 0$$
	$$H_1: b_j \not = 0$$
	$$\alpha \text{ - уровень значимости, который задан}$$
	
	Ошибки первого и второго рода:
	
	\begin{center}
		\begin{tabular}{|c|c|c|}
			\hline
			Гипотеза $H_0$ & верна & не верна \\
			\hline
			принята & нет ошибки  & \textbf{ошибка второго рода}  \\
			\hline
			отвергнута & \textbf{ошибка первого рода} & нет ошибки\\
			\hline
		\end{tabular}
	\end{center}
	
	$\alpha$ -- вероятность ошибки первого рода, $\beta$ -- вероятность ошибки второго рода.
	
	Критерий Стьюдента для оценки значимости параметра $b$:
	
	$$t_{\text{табл}}= t_\alpha = f(df; \alpha) > 0$$
	$$P(-t_\alpha < t < t_\alpha) = 1 - \alpha$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot031}
		\label{fig:screenshot031}
	\end{figure}
	
	Отвергаем нулевую гипотеу о незначимости:
	$$|t_b| \geqslant t_{\text{табл}} (=t_\alpha)$$
	$$|t_b| \rightarrow \alpha_b$$
	$$\alpha_b \leqslant \alpha$$
	
	Пусть: $$H_0: b_j = B_j$$
	
	Отвергаем нулевую гипотезу:
	
	$$\left|\dfrac{\hat{b}_j - B_j}{se(\hat{b}_j)}\right|\geqslant t_\alpha$$
	
	Каковы возможные значения $b_j$ -- каков доверительный интервал?
	
	$$\left|\dfrac{\hat{b}_j - B_j}{se(\hat{b}_j)}\right|\geqslant t_\alpha$$
	$$-t_\alpha < \dfrac{\hat{b}_j - B_j}{se(\hat{b}_j)} < t_\alpha$$
	$$\hat{b}_j - t_\alpha \cdot se(\hat{b}_j) < b_j < \hat{b}_j + t_\alpha \cdot se(\hat{b}_j)$$
	$$b_{j,\text{мин}} < b_j <b_{j,\text{макс}}$$
	$$P(b_{j,\text{мин}} < b_j <b_{j,\text{макс}}) = 1 - \alpha$$
	
	Пример: $x$ -- сумма счета в ресторане, евро; $y$ -- чаевые, евро. Получено уравнение регрессии:
	
	$$y = 0.492 + 0.126 x + \hat{\varepsilon}, \; n=20$$
	$$(se)\;(0.698)\;(0.020)$$
	$$\alpha = 0.05; \; df = 20 - 2 = 18 \Rightarrow t_\alpha = 2.1$$
	
	Выдвигаем гипотезы (нулевые и альтернативные) по двум параметрам:
	$$H_0: b_0 = 0$$
	$$H_1: b_0 \not = 0$$
	$$H_0: b_1 = 0$$
	$$H_1: b_1 \not= 0$$
	
	$$t_{b_0} = \dfrac{0.492 - 0}{0.698} = 0.7$$
	$$t_{b_1} = \dfrac{0.126 - 0}{0.020} = 6.3$$
	
	Свободный член незначим, коэффициент регрессии значим.
	
	$$b_{0,\text{мин}} = 0.492 - 2.1 \cdot 0.698 \approx -0.97$$
	$$b_{0,\text{макс}} = 0.492 + 2.1 \cdot 0.698 \approx 1.96$$
	$$-0.97 \leqslant b_0 \leqslant 1.96$$
	
	$$b_{1,\text{мин}} = 0.126 - 2.1 \cdot 0.020 \approx 0.084$$
	$$b_{1,\text{макс}} = 0.126 - 2.1 \cdot 0.020 \approx 0.168$$
	$$0.084 \leqslant b_1 \leqslant  0.168$$
	
	С вероятностью $0.95$ истинное значение $b_0$ ($b_1$) находится в соответствующих границах.
	
	Интерпретация коэффициента регрессии: с изменением суммы счета в ресторане на один евро, чаевые в среднем составят 0.126 евро, с вероятностью 0.95 они будут находиться в границах от 0.084 евро до 0.168 евро.
	
	$$P(t\geqslant t_{b_0}) = P(t \geqslant 0.7) = 1  - P(t<0.7) = 1 - F(0.7) \approx 1 -0.7536 = 0.2464$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot032}
		\label{fig:screenshot032}
	\end{figure}
	
	$$P(|t|\geqslant t_{b_0}) = P(t \geqslant 0.7) + P(t \leqslant 0.7)=  1 - F(0.7) + F(-0.7) \approx 1 -0.7536+0.2464 = 0.4927$$
	$$0.4928= \text{p-value} (\alpha_{b_0})$$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot033}
		\label{fig:screenshot033}
	\end{figure}
	
	Если $\alpha_{b_0} > \alpha$ -- незначимость свободного члена. В общем случае $\alpha_{b_0} > \alpha$ -- принимается $H_0: b_0 = 0$ либо $H_0: b_0 = B_0$
	
	Аналогично для коэффициента регрессии: $\alpha_{b_1} < \alpha$ -- значимость коэффициента регрессии. В примере $\alpha_{b_1} = 7 \cdot 10^{-6}<0.05$ -- отвергается нулевая гипотеза о незначимости.
	
	$$b_1 = ?$$
	$$0.084 \leqslant b_1 \leqslant  0.168$$
	
	Предположим:
	$$H_0: b_1 = 0.13$$
	$$H_1: b_1 \not = 0.13$$
	$$t_{b_1} = \dfrac{0.126 - 0.13}{0.020} = -0.2$$
	
	Нет оснований отвергнуть нулевую гипотезу.
	
	Вывод:
	$$H_0: b_j = B_j$$
	$$H_1: b_j \not = B_j$$
	
	Частный случай -- гипотеза о незначимости, в случае $B_j= 0$:
	$$H_0: b_j = 0 = B_j$$
	$$H_0: b_j \not = 0 = B_j$$
	
	
	\subsection{Односторонние t-тесты}
	Раньше было:
	$$H_0: b_j = B$$
	$$H_1: b_j \not = B$$
	
	Однако, возможны другие вопросы:
	$$H_0: b_j = B$$
	$$H_1: b_j = C$$
	
	или:
	$$H_0: b_j = B$$
	$$H_1: b_j >(<) B$$
	
	Пусть:
	$$H_0: b_j = B$$
	$$H_1: b_j = C$$
	$$B < C$$
	$$y = \hat{b}_0 + \hat{b}_1 \cdot x + \hat{\varepsilon}$$
	
	Вариант 1:
	$$B < \hat{b}_1 < C$$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.35\linewidth]{pics/screenshot036}
		\label{fig:screenshot036}
	\end{figure}
	
	Так как $|t_{b_1}| < t_\alpha$, то мы принимаем нулевую гипотезу с вероятностью $1-\alpha$.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.35\linewidth]{pics/screenshot037}
		\label{fig:screenshot037}
	\end{figure}
	
	Так как $|t_{b_1}| > t_\alpha$, то мы отклоняем нулевую гипотезу и склоняемся в сторону альтернативной
	с вероятностью $1-\alpha$.
	
	Вариант 2:
	$$B < C < \hat{b}_1$$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.35\linewidth]{pics/screenshot038}
		\label{fig:screenshot038}
	\end{figure}
	
	Так как $|t_{b_1}| > t_\alpha$, то мы отклоняем нулевую гипотезу и склоняемся в сторону альтернативной
	с вероятностью $1-\alpha$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot039}
		\label{fig:screenshot039}
	\end{figure}
	
	Так как $|t_{b_1}| < t_\alpha$, то мы принимаем нулевую гипотезу с вероятностью $1-\alpha$, что странно,
	так как $b_1$ ближе к $C$, чем к $B$ -- проблема в том, что мы выбрали слишком близкие
	друг к другу $C$ и $B$.
	
	Вариант 3:
	$$\hat{b}_1 < B < C $$
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot040}
		\label{fig:screenshot040}
	\end{figure}
	
	Так как $|t_{b_1}| < t_\alpha$, то мы принимаем нулевую гипотезу с вероятностью $1-\alpha$.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot041}
		\label{fig:screenshot041}
	\end{figure}
	
	В этой ситуации, как ни странно, принимаем нулевую гипотезу с вероятностью $1-\alpha$, несмотря на то, что
	$|t_{b_1}| > t_\alpha$, так как при любых $\hat{b}_1: \; \hat{b}_1 < B < C$ значение $\hat{b}_1$ будет ближе
	к $B$, чем к $C$. В этом проявляется односторонность t-теста. То есть, при $\hat{b}_1 < B < C$,
	если $t_{b_1} < 0$, то мы всегда принимаем нулевую гипотезу.
	
	Обобщим полученное:
	$$H_0: b_j = B$$
	$$H_1: b_j = C$$
	$$(B<C)$$
	
	Если $t_\alpha < t_{b_j}$, то мы отвергаем нулевую гипотезу, а если $t_\alpha > t_{b_j}$, то нет
	оснований отвергнуть нулевую гипотезу. Из за такого направления (либо до $t_\alpha$, либо после) такой
	тест называется односторонним t-тестом.
	$$(C<B)$$
	
	Если $t_{b_j}<-t_\alpha$, то мы отвергаем нулевую гипотезу, а если $-t_\alpha < t_{b_j}$, то нет
	оснований отвергнуть нулевую гипотезу.
	
	$$B<C$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.7\linewidth]{pics/screenshot046}
		\label{fig:screenshot046}
	\end{figure}
	$$C<B$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.7\linewidth]{pics/screenshot047}
		\label{fig:screenshot047}
	\end{figure}
	
	Аналитически:
	$$H_0: b_j = B$$
	$$H_1: b_j = C$$
	
	Если $B<C$, то $H_1: b_j = C \Rightarrow H_1: b_j > B$, а если $B>C$, то 
	$H_1: b_j = C \Rightarrow H_1: b_j < B$.
	
	Уровень значимости $\alpha$:
	\begin{itemize}
		\item Двусторонний t-критерий: $P(t < -t_\alpha) = P(t > t_\alpha) = \dfrac{\alpha}{2}$
		\item Односторонний t-критерий: или $B<C$ и $P(t>t_\alpha) = \alpha$, или
		$B>C$ и $P(t<-t_\alpha) = \alpha$.
	\end{itemize}
	
	Поэтому, если хотим взять $t_\alpha$ для одностороннего критерия, пользуясь таблицей
	двустороннего t-критерия, то нужно взять $\alpha$ в два раза больше (так как такое значение отсекает
	левые и правые области по 0.05).
	
	Пусть $B<\hat{b}_j<C$:
	$$H_0: b_j = B$$
	$$H_1: b_J = C$$
	
	Если $t_b<t_\alpha$, то принимаем $H_0$. В таком случае возможна ошибка второго рода -- принятие неверной
	нулевой гипотезы. В односторонних тестах можно определить вероятность такой ошибки, ошибки второго
	рода.
	$$B<C$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\linewidth]{pics/screenshot048}
		\label{fig:screenshot048}
	\end{figure}
	
	$$B<C$$
	$$t_c = ?$$
	$$H_0: b_j = B$$
	
	Пусть $\hat{b}_j = C$:
	$$t_c = \dfrac{C-B}{se(\hat{b}_j)} > 0$$
	
	Или, в другой системе координат, сдвинутой с началом вправо по оси абсцисс на $t_c$:
	$$H_0: b_j = C$$
	$$t_c = \dfrac{C-C}{se(\hat{b}_j)} = 0$$
	
	В данном случае думаем про нулевую гипотезу как $H_0: b_j = B$.
	$$B<C$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot049}
		\label{fig:screenshot049}
	\end{figure}
	
	У синего распределения $t_\alpha$ отсекает маленький хвост, соответствующий вероятности $\alpha$. Видно, что,
	если мы примемем при таком раскладе нулевую гипотезу, то, возможно, если нулевой гипотезой было бы
	$H_0: b_j = C$, то, возможно, мы отказались бы от такой нулевой гипотезы, и это было бы лучше. Возможно, мы примем неверную нулевую гипотезу, то есть совершим ошибку второго рода.
	$$H_0: b_j = B$$
	$$t_\alpha = \dfrac{b_\alpha - B}{se(\hat{b}_j)}$$
	$$b_\alpha = B+t_\alpha \cdot se(\hat{b}_j)$$
	
	Если нулевая гипотеза является обратной (для $B<C$):
	$$H_0: b_j = C$$
	$$t_{b_\alpha(c)} = \dfrac{b_\alpha - C}{se(\hat{b}_j)} < 0$$
	
	Хвост, отсекаемый $t_{b_\alpha(c)}$, у красного распределения, будет большим на графике в нашем случае (но в общем случае не факт).
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot050}
		\label{fig:screenshot050}
	\end{figure}
	
	Для того, чтобы выяснить, насколько велика вероятность выбрать неверную нулевую гипотезу: находим
	$t_\alpha$, после рассчитываем $t_{b_\alpha(c)}$. Тогда, вероятность
	ошибки второго рода $\gamma$ ($B<C$):
	$$\gamma = P(t < t_{b_\alpha(c)})$$
	
	Обратная величина, $1-\gamma$, называется \term{мощностью критерия}. Чем больше разнсть $C-B$, тем дальше друг от друга расположены
	распределения, тем меньше будет ошибка второго рода.
	$$B>C$$
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.4\linewidth]{pics/screenshot051}
		\label{fig:screenshot051}
	\end{figure}
	
	Вероятность
	ошибки второго рода $\gamma$ ($B>C$):
	$$\gamma = P(t \geqslant t_{b_\alpha(c)}) = 1 - P(t < t_{b_\alpha(c)})$$
	
	\subsubsection{Практическое использование односторонних t-тестов}
	
	Оценка стационарности временного ряда. Стационарный ряд:
	\begin{itemize}
		\item $MY_t = const$
		\item $DY_t = const$
		\item $cov(Y_t, Y_{t-\tau}) = f(\tau) \; \forall t, \tau$
	\end{itemize}
	
	Стационарные процессы:
	\begin{itemize}
		\item Процессы MA:
		$$y_t = \mu + \varepsilon_t - \theta_1 \varepsilon_{t-1} -
		\theta_2 \varepsilon_{t-2} - \cdots - \theta_q \varepsilon_{t-q}$$
		\item Процессы AR:
		$$y_t = a + c_1 y_{y-1} + \cdots + c_p y_{t-p} + \varepsilon_t$$
	\end{itemize}
	
	Частные случаи процессов:
	\begin{itemize}
		\item <<Белый шум>> -- частный случай стационарного процесса, когда $cov(Y_t,Y_{t-1})=0$
		\item Случайное блуждание: $y_t = y_{t-1} + \varepsilon_t$
		\item Условие стационарности: корни уравнения (включая комплексные)
		$$1-c_1 z- c_2 z^2 - \cdots - c_p z^p = 0$$
		должны лежать вне <<единичного корня>>, то есть $|z|>1$
	\end{itemize}
	
	Нестационарные процессы:
	\begin{itemize}
		\item С детерминированным трендом: $y_t = b_0 + b_1 t + \varepsilon_t$
		\item Со стохастическим трендом: $y_t = c_1 y_{t-1} + \varepsilon_t$ или 
		$y_t = b_0 +  c_1 y_{t-1} + \varepsilon_t$
		\item С детерминированным и стохастическим трендом: $y_t = b_0 + b_1 t + c_1 y_{t-1} +
		\varepsilon_t$
	\end{itemize}
	
	Условие стационарности для $y_t = c_1 y_{t-1} + \varepsilon_t$ при $c_1 < 1$, если
	$c_1 = 1$, то случайное блуждаение, если $c_1 > 1$, то <<взрывной>> характер
	динамики.
	
	По факту, проверяем, либо $c_1 = 1$, либо $c_1 > 1$. То есть, полная аналогия с односторонним t-тестом. Возникает проблема, которая заключается в том, что, если коэффициент
	$c_1$ близок к единице, то есть, если гипотеза нулевая верна, то t-статистика не распределена
	по закону Стьюдента. Эта проблема решается -- проведем преобразование:
	$$y_t = c_1 y_{t-1} + \varepsilon_t$$
	$$y_t - y_{t-1} = c_1 y_{t-1} + \varepsilon_t - y_{t-1}$$
	$$\mathit{\Delta} y_t = (c_1 - 1) y_{t-1} + \varepsilon_t$$
	$$\mathit{\Delta} y_t = \delta y_{t-1} + \varepsilon_t$$
	
	Это преобразование привело к тому, что в нестационарном процессе коэффициент $\delta$ при
	$y_{t-1}$ будет равен не единице, а нулю. Тогда:
	$$H_0: \delta = 0$$
	$$H_0: \delta < 0$$
	
	Однако, распределение такой случайной величины, по прежнему, не подчиняется распределению
	Стьюдента, но для этого распределения были рассчитаны критические значения Дики-Фуллером.
	
	Тест Дики-Фуллера.  
	
	Критические значения: $DF$ статистика. Стационарность:
	$$t<DF_l$$
	
	Тест Дики-Фуллера проверяет во временном ряду наличие единичного корня -- то есть он проверяет нестационарность (единичный корень $\approx$ нестационарность), а если гипотеза отвергается, то ряд считается стационарным. Если хотим
	получить стационарный ряд, то p-value должно быть достаточно маленьким, чтобы мы отвергли нулевую гипотезу о нестационарности. Стационарность:
	
	$$t<DF$$
	
	Расширенный тест Дики-Фуллера ($ADF$).
	
	$$\Delta y_t = \delta \cdot y_{t-1} + \sum_{j=1}^{p-1}(\alpha_j \cdot \Delta y_{t-j})+\varepsilon_t$$
	
	Где $\delta = c_1 + c_2 +\cdots + c_p - 1$
	$$H_0: \delta = 0$$
	$$H_1: \delta < 0$$
	
	\subsection{Статистические характеристики оценок параметров уравнения регрессии}
	
	Для изучения этих характеристик требуется матричная запись уравнения регрессии:
	$$Y = X \cdot \hat{B} + \hat{E}$$
	
	Вектор $\hat{B}$ является вектором оценок истинных значений, поэтому они подвержены случайным колебаниям, то есть являются случайными величинами:
	
	$$M\hat{B} = ?$$
	$$D\hat{B} = \sigma^2_{\hat{B}} = ?$$
	
	Матожидание оценок использовалось в препосылках классической нормальной линейной модели, а дисперсия оценок использовалась в расчете
	критерия Стьюдента.
	
	Из МНК получаем вектор оценок параметров:
	$$\hat{B} = (X^T \cdot X)^{-1} \cdot X^T \cdot Y$$
	$$\hat{B} = (X^T \cdot X)^{-1} \cdot X^T \cdot (X\cdot B + E)$$
	
	Предполагаем, что мы правильно угадали параметры истинной модели (и состав переменных, и функциональную форму):
	
	$$M\hat{B} = M ((X^T \cdot X)^{-1} \cdot X^T \cdot (X\cdot B + E)) = 
	M ((X^T \cdot X)^{-1} \cdot( X^T \cdot X)\cdot B) +$$ 
	$$ + M ((X^T \cdot X)^{-1} \cdot X^T \cdot E) = MB +  
	(X^T \cdot X)^{-1} \cdot X^T \cdot ME = B + 0 = B $$
	
	Оценка дисперсии:
	$$D\hat{B} = \sigma_{\hat{B}}^2 = ?$$
	$$cov(\hat{B}) = M\left((\hat{B}-M\hat{B}) \cdot (\hat{B} - M\hat{B})^T\right) =$$
	$$=M\left(((X^T \cdot X)^{-1} \cdot X^T \cdot (X \cdot B + E) - B) \cdot 
	((X^T \cdot X)^{-1} \cdot X^T \cdot (X \cdot B + E) - B)^T\right) =$$
	$$ = M((B+(X^T \cdot X)^{-1}\cdot X^T \cdot E - B) \cdot 
	(B+(X^T \cdot X)^{-1}\cdot X^T \cdot E - B)^T) = $$
	$$ = M(((X^T \cdot X)^{-1}\cdot X^T \cdot E) \cdot 
	((X^T \cdot X)^{-1}\cdot X^T \cdot E)^T) = $$
	$$= (X^T \cdot X)^{-1} \cdot X^T \cdot M(E\cdot E^T) \cdot X \cdot (X^T\cdot X)^{-1}$$
	
	$$M(E\cdot E^T) = ?$$
	$$M(E \cdot E^T) = M((E-ME)\cdot (E-ME)^T) = cov(E) = \Omega$$
	
	
	Матрица ковариации случайных остатков $\Omega$:
	\begin{equation*}
		\Omega = \left(
		\begin{array}{cccc}
			\sigma^2_{\varepsilon_1} & cov(\varepsilon_1, \varepsilon_2) & \ldots &  cov(\varepsilon_1, \varepsilon_n)\\
			cov(\varepsilon_2, \varepsilon_1) & \sigma^2_{\varepsilon_2} & \ldots &  cov(\varepsilon_2, \varepsilon_n)\\
			\vdots & \vdots & \ddots & \vdots\\
			cov(\varepsilon_n, \varepsilon_1) &  cov(\varepsilon_n, \varepsilon_2) & \ldots & \sigma^2_{\varepsilon_n}
		\end{array}
		\right)
	\end{equation*}
	
	При гомоскедастичности и некоррелированности случайных остатков:
	
	\begin{equation*}
		\Omega = \left(
		\begin{array}{cccc}
			\sigma^2_{\varepsilon} & 0 & \ldots & 0\\
			0 & \sigma^2_{\varepsilon} & \ldots & 0\\
			\vdots & \vdots & \ddots & \vdots\\
			0 &  0 & \ldots & \sigma^2_{\varepsilon}
		\end{array}
		\right) = \sigma_\varepsilon^2 \cdot I
	\end{equation*}
	
	$$cov(\hat{B}) = (X^T \cdot X)^{-1} \cdot X^T \cdot M(E\cdot E^T) \cdot X \cdot (X^T\cdot X)^{-1} =
	(X^T \cdot X)^{-1} \cdot X^T \cdot\sigma^2_{\varepsilon} \cdot I \cdot X \cdot (X^T\cdot X)^{-1}=$$
	$$ =\sigma^2_{\varepsilon} \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot X \cdot (X^T\cdot X)^{-1}=
	\sigma^2_{\varepsilon} \cdot (X^T \cdot X)^{-1}$$
	
	Таким образом:
	$$cov(\hat{B}) = (X^T \cdot X)^{-1} \cdot \sigma^2_{\varepsilon}$$
	$$cov(\hat{b}_j,\hat{b}_j) = \sigma^2_{\hat{b}_j}$$
	$$\sigma^2_{\varepsilon} = ?$$
	
	Для того, чтобы иметь дисперсию коэффициентов регрессии, необходимо найти истинную дисперсию остатков.
	$$\hat{E} = Y - X\cdot \hat{B} = Y - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot Y=$$
	$$ = (X \cdot B + E) - X \cdot\left( (X^T \cdot X)^{-1} \cdot X^T \cdot (X \cdot B + E)\right)=$$
	$$ = X \cdot B + E - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot X \cdot B - 
	X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E =$$
	$$ = X \cdot B + E - X \cdot B - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E=$$
	$$ = E  - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E$$
	
	Теперь найдем матожидание произведения векторов случайных остатков:
	$$M(\hat{E}^T \cdot \hat{E}) = M\left(( E  - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E)^T 
	\cdot  (E  - X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E)\right) = $$
	$$=M(E^T \cdot E - E^T \cdot X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E - 
	E^T \cdot X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E +$$
	$$+ E^T \cdot X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E)=$$
	$$=M\left(E^T \cdot E - E^T \cdot X \cdot (X^T \cdot X)^{-1} \cdot X^T \cdot E\right)=$$
	
	
	Уменьшаемое:
	$$M(E^T \cdot E) = M((E - ME)^T \cdot (E - ME)) = n \cdot \sigma^2_{\varepsilon}$$
	
	Вычитаемое:
	$$X \cdot (X^T \cdot X)^{-1} \cdot X^T \equiv C (n \times n)$$
	$$M(E^T \cdot C \cdot E ) = M\left(\sum_{i,j=1}^{n}(c_{ij}\cdot \varepsilon_j \cdot \varepsilon_j)\right)=
	\sum_{i,j=1}^{n}c_{ij}\cdot M(\varepsilon_i \cdot \varepsilon_j) = $$
	$$ = \sum_{i,j=1}^{n}c_{ij}\cdot cov(\varepsilon_i,\varepsilon_j) = 
	\sum_{i=1}^{n}c_{ii}\cdot cov(\varepsilon_i,\varepsilon_i) = 
	\sum_{i=1}^{n}c_{ii}\cdot \sigma_{\varepsilon}^2 =  \sigma_{\varepsilon}^2 \sum_{i=1}^{n}c_{ii}  = \sigma_{\varepsilon}^2 \cdot tr(C) =$$
	$$ = \sigma_{\varepsilon}^2 \cdot tr(X \cdot (X^T \cdot X)^{-1} \cdot X^T) = 
	\sigma_{\varepsilon}^2 \cdot tr((X^T \cdot X)^{-1} X^T \cdot X) = 
	\sigma_{\varepsilon}^2 \cdot tr(E_{m+1}) = (m+1) \cdot \sigma_{\varepsilon}^2$$
	
	Таким образом:
	$$M(\hat{E}^T \cdot \hat{E}) = n \cdot \sigma^2_{\varepsilon} - (m+1) \cdot \sigma^2_{\varepsilon} = 
	(n-m-1) \cdot \sigma^2_{\varepsilon}$$
	
	С другой стороны:
	$$M(\hat{E}^T \cdot \hat{E}) = \sum_{i=1}^{n} \hat{\varepsilon}_i^2 = SS_{\text{ост}}$$
	
	Тогда получаем оценку дисперсии случайных остатков:
	
	$$\hat{\sigma}_\varepsilon^2 = \dfrac{\sum_{i=1}^{n}\hat{\varepsilon}_i^2}{n-m-1} =
	\dfrac{SS_{\text{ост}}}{n-m-1} = s^2_e$$
	
	Вывели раньше:
	$$cov(\hat{B}) = (X^T \cdot X)^{-1} \cdot \hat{\sigma}_\varepsilon^2$$
	$$\hat{\sigma}_{\hat{b}_j} = \hat{\sigma}_\varepsilon^2 \cdot [(X^T \cdot X)^{-1}]_{jj} \;\;\;\; 
	j = 0,1, \cdots, m$$
	
	Например, для $y = \hat{b}_0 + \hat{b}_1 \cdot x + \hat{\varepsilon}$:
	
	\begin{equation*}
		X^T \cdot X = \left(
		\begin{array}{cc}
			n & \sum\limits_{i=1}^{n}x_i \\
			\sum\limits_{i=1}^{n}x_i & \sum\limits_{i=1}^{n}x_i^2\\
		\end{array}
		\right)
	\end{equation*}
	
	Диагональные элементы матрицы $(X^T \cdot X)^{-1}$:
	$$jj = 11 \Rightarrow \dfrac{n}{|X^T \cdot X|} = \dfrac{n}{n\sum_{i=1}^{n}x_i^2 - (\sum_{i=1}^{n}x_i)^2}$$
	$$= \dfrac{n}{n^2 \cdot \sigma^2_x} = \dfrac{1}{\sum_{i=1}^{n}(x_i - \overline{x})^2}$$
	
	$$[(X^T \cdot X)^{-1}] _{11} = \dfrac{1}{\sum_{i=1}^{n}(x_i-\overline{x})^2}$$
	$$\hat{\sigma}^2_{\hat{b}_1} = \dfrac{\sum_{i=1}^{n}(y_i-\hat{y}_i)^2/(n-2)}{\sum_{i=1}^{n}(x_i-\overline{x})^2}$$
	$$\hat{\sigma}_{\hat{b}_1} = se(\hat{b}_1) =
	\sqrt{\dfrac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2/(n-2)}{\sum_{i=1}^{n}(x_i - \overline{x})^2}}$$
	
	Диагональные элементы матрицы $(X^T \cdot X)^{-1}$:
	$$jj = 00 \Rightarrow \dfrac{\sum_{i=1}^{n}x_i^2}{|X^T \cdot X|} = \dfrac{\sum_{i=1}^{n}x_i^2}{n\sum_{i=1}^{n}x_i^2 - (\sum_{i=1}^{n}x_i)^2}$$
	$$= \dfrac{\sum_{i=1}^{n}x_i^2}{n^2 \cdot \sigma^2_x} = \dfrac{\sum_{i=1}^{n}x_i^2}{n \cdot 
		\sum_{i=1}^{n}(x_i - \overline{x})^2}$$
	
	$$[(X^T \cdot X)^{-1}] _{00} = 
	\dfrac{\sum_{i=1}^{n}x_i^2}{n \cdot \sum_{i=1}^{n}(x_i - \overline{x})^2}$$
	$$\hat{\sigma}^2_{\hat{b}_0} = \dfrac{\sum_{i=1}^{n}x_i^2 \cdot \sum_{i=1}^{n}(y_i-\hat{y}_i)^2/(n-2)}
	{n \cdot \sum_{i=1}^{n}(x_i - \overline{x})^2}$$
	$$\hat{\sigma}_{\hat{b}_0} = se(\hat{b}_0) =
	\sqrt{\dfrac{\sum_{i=1}^{n} x_i^2 \cdot \sum_{i=1}^{n}(y_i - \hat{y}_i)^2/(n-2)}
		{n \cdot \sum_{i=1}^{n}(x_i - \overline{x})^2}}$$
	
	К вопросу об использовании t-статистики для проверки гипотез о значениях параметров регрессии.
	
	$$\hat{b}_j \sim N(b_j, \sigma^2_{b_j})$$
	
	Где $\sigma^2_{b_j}$ -- истинная дисперсия $\hat{b}_j$, а $se^2(\hat{b}_j)$ -- оценка дисперсии $\hat{b}_j$.
	
	$$\dfrac{\hat{b}_j - B_j}{\sigma_{b_j}} \sim N(0,1)$$
	
	Можно показать, что:
	$$\dfrac{SS_{\text{ост}}}{\sigma^2_\varepsilon} = 
	\dfrac{\hat{\sigma}^2_\varepsilon \cdot (n-m-1)}{\sigma^2_\varepsilon} \sim \chi^2(n-m-1)$$
	
	Тогда:
	
	$$\dfrac{\hat{b}_j - B_j}{\sigma_{b_j}} \Big/ \sqrt{
		\dfrac{\hat{\sigma}^2_\varepsilon \cdot (n-m-1)}
		{\sigma^2_\varepsilon \cdot (n-m-1)}} \sim t(n-m-1)$$
	
	$$\dfrac{(\hat{b}_j - B_j) \cdot \sigma_\varepsilon}{\sigma_{b_j}\cdot\hat{\sigma}_\varepsilon} \sim t(n-m-1)$$
	
	Так как:
	$$\dfrac{\sigma_{b_j}}{\sigma_\varepsilon} = \dfrac{se(b_j)}{\hat{\sigma}_\varepsilon}
	\Rightarrow \dfrac{\hat{b}_j - B_j}{se(b_j)} \sim t(n-m-1)$$
	
	\subsection{Оценка значимости регрессии (тренда)}
	
	Проверка значимости уравнения регрессии (тренда):
	$$H_0: b_1 = b_2 = \cdots = b_p =0$$
	$$H_1: \exists b_j \not =0$$
	
	Суммы квадратов при выполнении нулевой гипотезы $H_0$:
	$$SS_\text{ост} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 = 
	\sum_{i=1}^{n} (y_i - b_0)^2 =  \sum_{i=1}^{n} (y_i - (\overline{y} - b_1 \cdot \overline{x}))^2 = \sum_{i=1}^{n} (y_i - \overline{y})^2 =
	SS_\text{общ}$$	
	$$SS_\text{общ} = \sum_{i=1}^{n} (\hat{y}_i - \overline{y})^2 = 
	\sum_{i=1}^{n} (b_0 - \overline{y})^2 = \sum_{i=1}^{n} (\overline{y} - \overline{y})^2 = 0$$
	
	Если $H_0: b_1 = b_2 = \cdots = b_p = 0$, то
	$$\dfrac{SS_\text{ост}}{\sigma^2_\varepsilon} \sim \chi^2(n-m-1)$$
	$$\dfrac{SS_\text{факт}}{\sigma^2_\varepsilon} \sim \chi^2(m)$$
	
	Тогда:
	$$\dfrac{SS_\text{факт}}{\sigma^2_\varepsilon \cdot m} \big/ \dfrac{SS_\text{ост}}{\sigma^2_\varepsilon \cdot (n-m-1)} =
	\dfrac{SS_\text{факт} \cdot (n-m-1)}{SS_\text{ост}\cdot m} = 
	\dfrac{SS_\text{факт}}{SS_\text{ост}} \cdot \dfrac{n-m-1}{n} \sim F(m, n-m-1)$$
	
	Критерий Фишера:
	$$F = \dfrac{DS_\text{факт}}{DS_\text{ост}} = 
	\dfrac{SS_\text{факт} \cdot (n-m-1)}{SS_\text{ост}\cdot m} = 
	\dfrac{SS_\text{факт}}{SS_\text{ост}} \cdot \dfrac{n-m-1}{n}$$
	
	Если нулевая гипотеза выполняется, то $F$-статистика имеет распределение Фишера, поэтому
	в качестве табличного значения мы берем $F_\text{табл}(\alpha;m;n-m-1)$ и смотрим, насколько полученное распределение похоже на распределение Фишера.
	
	Плотность распределения Фишера при степенях свободы $(m=1,n-m-1=20)$:
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.3\linewidth]{pics/screenshot052}
		\label{fig:screenshot052}
	\end{figure}
	
	Плотность распределения Фишера при степенях свободы $(m=3,n-m-1=20)$:
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.35\linewidth]{pics/screenshot053}
		\label{fig:screenshot053}
	\end{figure}

	Плотность распределения Фишера при степенях свободы $(m=18,n-m-1=20)$:
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.35\linewidth]{pics/screenshot054}
		\label{fig:screenshot054}
	\end{figure}
	
	Так как справедливы следующие выражения:
	$$R^2 = \dfrac{SS_\text{факт}}{SS_\text{общ}} = 1 - \dfrac{SS_\text{ост}}{SS_\text{общ}}$$
	$$SS_\text{факт} = R^2 \cdot SS_\text{общ}$$
	$$SS_\text{ост} = (1-R^2) \cdot SS_\text{общ}$$
	
	То, подставляя эти выражения в формулу $F$-статистики, получаем, что:
	
	$$F = \dfrac{R^2 \cdot SS_\text{общ}}{(1-R^2) \cdot SS_\text{общ}} \cdot \dfrac{n-m-1}{m}$$
	$$F = \dfrac{R^2}{1-R^2} \cdot \dfrac{n-m-1}{m}$$
	
	Таким образом, если $b_1 = b_2 = \cdots = b_p = 0$, то $y = b_0 + 0 \cdot x_1 + 
	\cdots 0 \cdot x_p + \varepsilon = b_0 + \varepsilon$ (то есть уравнение выглядит как стационарный ряд, незначимо влияние $x$ на $y$). Тогда:
	$$R^2 = \dfrac{\sum\limits_{i=1}^n(\hat{y}_i - \overline{y})^2}
	{\sum\limits_{i=1}^n(y_i - \overline{y})^2} = 
	\dfrac{\sum\limits_{i=1}^n(b_0 - b_0)^2}
	{\sum\limits_{i=1}^n(y_i - \overline{y})^2} = 0$$
	
	То есть, можно сказать, что нулевая гипотеза о равенстве в истинной модели всех параметров нулю эквивалентна нулевой гипотезе о незнаимости коэффициента детерминации:
	$$H_0: b_1 = \cdots = b_p = 0 \Longleftrightarrow
	 H_0: R^2 = 0$$
	 
	 Для $y = \hat{b}_0 + \hat{b}_1 \cdot x + \hat{\varepsilon}$:
	 $$t_{b_1} = \dfrac{\hat{b}_1}{se(b_1)}$$
	 
	 Так как
	 $$se(b_1) = \dfrac{SS_\text{ост}}{(n-2) \cdot \sum\limits_{i=1}^{n}(x_i - \overline{x})^2}$$
	 то:
	 $$t_{b_1}^2 = \dfrac{\hat{b}_1^2 \cdot (n-2) \cdot \sum\limits_{i=1}^{n}(x_i - \overline{x})^2}{SS_\text{ост}} =  
	 \dfrac{\hat{b}_1^2 \cdot (n-2) \cdot \sigma^2_x \cdot n}{SS_\text{ост}}$$
	 
	 Также, мы знаем, что:
	 $$\hat{r} = \hat{b}_1 \cdot \dfrac{\sigma_x}{\sigma_y}, \;\;\; \hat{b}_1 = \hat{r}
	  \cdot \dfrac{\sigma_y}{\sigma_x}$$
	  
	 Откуда, подставляя $\hat{b}_1$, получим
	 $$t_{b_1}^2 = 
	 \dfrac{\hat{r}^2 \cdot \sigma_y^2  \cdot (n-2) \cdot \sigma^2_x \cdot n}
	 {\sigma^2_x \cdot SS_\text{ост}} = 
	 \dfrac{\hat{r}^2 \cdot \sigma_y^2  \cdot (n-2)  \cdot n}
	 {SS_\text{ост}} = 
	 \dfrac{R^2 \cdot \sigma_y^2  \cdot (n-2)  \cdot n}
	 {SS_\text{ост}}  = \dfrac{SS_\text{факт} \cdot \sigma_y^2  \cdot (n-2)  \cdot n}
	 {SS_\text{общ} \cdot SS_\text{ост}} =$$
	 $$ =  \dfrac{SS_\text{факт} \cdot \sigma_y^2  \cdot (n-2)  \cdot n}
	 {\sigma^2_y \cdot n \cdot SS_\text{ост}} = 
	 \dfrac{SS_\text{факт} \cdot (n-2) }
	 {SS_\text{ост}}  = \dfrac{SS_\text{факт}}{SS_\text{ост}} \cdot \dfrac{n - 1 - 1}{1} = F$$
	 
	 \subsection{Последствия неправильной спецификации модели}
	Спецификация модели предполагает:
	\begin{enumerate}
		\item Выбор независимых переменных;
		\item Выбор функциональной формы модели.
	\end{enumerate}
	
	Каковы последствия, если функциональная форма не соответствует реальному экономическому явлению? Будет:
	\begin{enumerate}
		\item Ошибки интерпретации и прогнозирования;
		\item Нарушения требований к случайным остаткам;
		\item Недостоверность оценок параметров.
	\end{enumerate}
	
	Каковы последствия, если будет 
	несоответствие состава независимых переменных <<реальной>> модели:
	\begin{enumerate}
		\item Невключение существенных переменных;
		\item Включение несущественных переменных.
	\end{enumerate}
	
	Начнем рассматривать вопрос со случая невключения существенных переменных.
	<<Реальная>> модель:
	$$y = b_0 + b_1 \cdot x_1 + \cdots + a_1 \cdot z_1 + \cdots + \varepsilon$$
	$$Y = X \cdot B + Z \cdot A + E$$
	
	Построена модель:
	$$y = \hat{b}_0 + \hat{b}_1 \cdot x_1 + \cdots + \hat{\varepsilon}$$
	$$Y = X \cdot \hat{B} + \hat{E}$$
	
	
	
	
\end{document}
